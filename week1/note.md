## Machine Translation
### 为什么需要 LSTM？
传统 RNN 虽然能处理序列数据（如文本、时间序列等），但在学习长序列时，早期信息会随着网络层数增加逐渐「遗忘」，导致无法捕捉远距离的依赖关系（比如一句话中，开头的某个词和结尾的某个词可能存在逻辑关联，但 RNN 难以记住这种关联）。

LSTM 通过设计特殊的「门控机制」解决了这个问题，能够有选择地「记住」重要信息、「遗忘」无关信息，并根据新输入更新记忆。


### LSTM 的核心结构
LSTM 的核心是「细胞状态（Cell State）」，可以理解为一条贯穿网络的「信息传送带」，负责长期保存关键信息。同时，LSTM 通过三个门控单元控制信息的流动：

1. **遗忘门（Forget Gate）**  
   决定哪些信息从细胞状态中被丢弃。通过 sigmoid 函数输出 0~1 之间的值（0 表示完全遗忘，1 表示完全保留），过滤掉无关信息。

2. **输入门（Input Gate）**  
   决定哪些新信息被存入细胞状态。分为两步：  
   - 用 sigmoid 函数选择需要更新的信息；  
   - 用 tanh 函数生成候选更新值（范围 -1~1）；  
   最终将两者结合，更新细胞状态。

3. **输出门（Output Gate）**  
   决定当前时刻的输出。用 sigmoid 函数选择细胞状态中需要输出的部分，再通过 tanh 函数处理细胞状态并与 sigmoid 结果相乘，得到最终输出。


### LSTM 的优势与应用
- **优势**：能有效处理长序列数据，保留长期依赖关系，避免梯度问题。  
- **应用场景**：  
  - 自然语言处理（机器翻译、文本生成、情感分析）；  
  - 时间序列预测（股票价格、天气、销量预测）；  
  - 语音识别、手写体识别等。


### 总结
LSTM 是 RNN 的改良版本，通过门控机制和细胞状态实现了对长序列信息的有效记忆和处理，是深度学习中处理序列数据的重要工具。后续还衍生出了更简化的变体（如 GRU，门控循环单元），但 LSTM 仍在许多场景中被广泛使用。

## semantic segmentation｜object detection
语义分割（Semantic Segmentation）和目标检测（Object Detection）是计算机视觉中的两种任务，区别如下：

- **目标检测**：识别图像中的每个目标，并用边界框（bounding box）标记出来。输出是每个目标的位置（框坐标）和类别。例如，检测出图像中的猫和狗，并用矩形框圈出它们。

- **语义分割**：对图像中的每个像素进行分类，输出每个像素属于哪个类别。结果是一张与原图同尺寸的“标签图”，每个像素都被赋予一个类别标签。例如，把图像中的所有猫像素标记为“猫”，所有狗像素标记为“狗”。

**总结**：  
- 目标检测关注“哪里有目标”，输出边界框和类别。  
- 语义分割关注“每个像素是什么”，输出像素级的类别标签。


## cross-entropy loss
**交叉熵损失（cross-entropy loss）** 是机器学习和深度学习中常用的损失函数，主要用于分类任务。

它衡量模型预测的概率分布与真实标签分布之间的差异。交叉熵越小，说明模型预测越接近真实标签。

公式（以二分类为例）：
```
L = -[y * log(p) + (1-y) * log(1-p)]
```
其中  
- `y` 是真实标签（0或1）  
- `p` 是模型预测为1的概率

**总结**：交叉熵损失用于衡量分类模型的预测准确性，损失越小，模型越好。


## ResNet
ResNet（Residual Network，残差网络）是一种深度卷积神经网络，由微软研究院于2015年提出。它通过引入“残差连接”（skip connection），解决了深层网络训练中的梯度消失和退化问题。

核心思想：  
- 在传统卷积层之间增加“跳跃连接”，让输入可以直接绕过若干层传到后面。  
- 网络学习的是“残差”（即输出与输入的差值），而不是直接学习输出。

优点：  
- 能训练非常深的网络（如ResNet-50、ResNet-101等），效果优于传统结构。  
- 在图像分类、检测等任务中表现优异。

结构示意：  
```
输出 = F(x) + x
```
其中 F(x) 是卷积层的输出，x 是输入。

**总结**：ResNet 通过残差连接让深层网络更易训练，是现代计算机视觉任务中的主流架构之一。